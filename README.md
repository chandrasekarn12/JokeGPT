# ğŸ¤– JokeGPT  
*A GPT-style Transformer trained to generate dad jokes from Reddit*

[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Enabled-red?logo=pytorch)](https://pytorch.org/)
[![Transformer](https://img.shields.io/badge/Model-GPT--mini-green)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ§  Overview

**JokeGPT** is a lightweight GPT-style transformer model, inspired by GPT-1, trained from scratch on hundreds of thousands of Reddit dad jokes. The model will learn to understand structure, style, and punchline timing, generating original and often hilariously bad dad jokes with each run.

This project highlights end-to-end ML engineering:
- Dataset cleaning and tokenization
- Training a character-level language model
- Saving checkpoints and visualizing loss curves
- Deploying the model for joke generation

---

## ğŸ¯ Project Objectives

- Build a character-level GPT-style Transformer model trained on real-world humor data (Reddit dad jokes)
- Demonstrate a full ML workflow: data cleaning, tokenization, training, evaluation, and inference
- Provide an interpretable, minimal GPT implementation using PyTorch and no external modeling libraries
- Enable reproducible joke generation with saved checkpoints and a simple generation interface
- Offer a lightweight, extensible framework for experimenting with Transformer architectures on small text datasets

---

## ğŸ“¦ Technologies Used

- **Python 3.10+**
- **PyTorch** â€“ Transformer implementation
- **NumPy / Pandas** â€“ data handling
- **Matplotlib** â€“ training curve visualization
- **Reddit Dad Jokes Dataset** â€“ collected from Kaggle

---

## ğŸ“ Project Structure

```bash
JokeGPT/
â”œâ”€â”€ config.py           # All model & training hyperparameters
â”œâ”€â”€ prepare_data.py     # Loads, cleans, tokenizes dad jokes into .bin format
â”œâ”€â”€ model.py            # GPTConfig + Transformer blocks (self-attn, FFN, etc.)
â”œâ”€â”€ train.py            # Full training loop + evaluation + checkpointing
â”œâ”€â”€ generate.py         # Loads trained model and generates jokes
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reddit_dadjokes.csv   # Raw joke data (from Kaggle)
â”‚   â”œâ”€â”€ train.bin              # Encoded training data
â”‚   â”œâ”€â”€ val.bin                # Encoded validation data
â”‚   â”œâ”€â”€ meta.pkl               # Vocabulary + encoders
â”‚   â”œâ”€â”€ checkpoint.pt          # Final trained weights
â”‚   â””â”€â”€ loss_curve.png         # Training loss visualization
